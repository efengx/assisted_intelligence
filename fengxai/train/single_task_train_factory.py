import numpy as np
import evaluate
import os
import torch
# TODO: åç»­å¯ä»¥è€ƒè™‘ä½¿ç”¨ transformers ä¸­çš„æ–¹æ³•æ›¿ä»£æ‰ï¼Œå‡å°‘é¢å¤–çš„ä¾èµ–
import graphviz
# import bitsandbytes as bnb
import transformers
import datasets

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from peft import get_peft_model, LoraConfig, TaskType, PromptEncoderConfig
from accelerate import Accelerator
from torch.utils.data.dataloader import DataLoader
from torch import nn
from transformers.trainer_pt_utils import get_parameter_names
from fengxai.utils.print_utils import print_summary, print_source
from transformers import (
    AutoTokenizer,
    Trainer,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM
)
from huggingface_hub import (
    login,
    HfApi,
    create_repo,
)
from torch.utils.tensorboard import SummaryWriter
# from torchview import draw_graph
# from datetime import datetime
from fengxai.utils.log_center import create_logger
from fengxai.train.prof_callback import ProfCallback
from fengxai.dataset.build_dataset import maker_data_module

logger = create_logger()

# è®¾ç½®transformersçš„æ—¥å¿—
transformers.utils.logging.set_verbosity_debug()
transformers.utils.logging.enable_default_handler()
transformers.utils.logging.enable_explicit_format()
transformers.tokenization_utils.logging.set_verbosity_debug()
datasets.utils.logging.set_verbosity_debug()

def single_task_train(
        model_args, data_args, training_args
    ):

    logger.info("***************** è½½å…¥æ¨¡å‹: åˆ†è¯å™¨; æ¨¡å‹ *****************")
    print_source()
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        use_auth_token=model_args.hf_hub_token if model_args.hf_hub_token and model_args.use_auth_token else None,
    )
    logger.info(f"tokenizer={tokenizer}")
    
    """
    AutoModelForSequenceClassification ç±»åŠ è½½ DistilBertForSequenceClassification ç±»ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚ 
    ç”±äº AutoModelForSequenceClassification ä¸æ¥å—å‚æ•°â€œnum_labelsâ€ï¼Œ
    å®ƒè¢«ä¼ é€’ç»™æ¥å—å®ƒçš„åŸºç¡€ç±» DistilBertForSequenceClassificationã€‚
    
    to("cuda") è¡¨ç¤ºä½¿ç”¨ gpu åŠ è½½
    """
    if model_args.task_type == 'text-generation':
        # sentiment-analysis
        # id2labelåœ¨å¼€å§‹è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œä½¿ç”¨å’Œåˆ›å»ºé¢„æœŸ ID åˆ°å…¶æ ‡ç­¾çš„æ˜ å°„label2idï¼š
        id2label = {0: "aiå†™çš„", 1: "äººç±»å†™çš„"}
        label2id = {"aiå†™çš„": 0, "äººç±»å†™çš„": 1}

        model=AutoModelForSequenceClassification.from_pretrained(
            model_args.model_name_or_path,
            # å¿…é¡»çš„ï¼Œä½œç”¨ï¼šæ›¿æ¢æ‰åŸæ¥çš„æ¨¡å‹çš„åˆ†ç±»å¤´éƒ¨ï¼Œæ ¹æ®è®­ç»ƒçš„æ•°æ®é›†é‡æ–°ç”Ÿæˆæ–°çš„äºŒåˆ†ç±»å¤´éƒ¨
            num_labels=2,
            id2label=id2label,
            label2id=label2id,
            use_auth_token=model_args.hf_hub_token if model_args.hf_hub_token and model_args.use_auth_token else None,
        )
    elif model_args.task_type == 'text2text':
        model=AutoModelForCausalLM.from_pretrained_model(
            model_args.model_name_or_path,
            use_auth_token=model_args.hf_hub_token if model_args.hf_hub_token and model_args.use_auth_token else None,
        )

    if training_args.do_peft:
        logger.info("********* ä½¿ç”¨ peft è®­ç»ƒ ***********")
        peft_config = PromptEncoderConfig(
            task_type="SEQ_CLS", 
            num_virtual_tokens=20, 
            encoder_hidden_size=128 
        )
        model=get_peft_model(model, peft_config)

        logger.info("********** æ¨¡å‹å¯è®­ç»ƒå‚æ•°é‡ *************")
        # AttributeError: 'LongformerForSequenceClassification' object has no attribute 'print_trainable_parameters'
        model.print_trainable_parameters()

    num=model.num_parameters()
    logger.info(f"model å‚æ•°é‡ï¼š{num}")
    logger.info(f"model config: {model.config}")

    # accelerator = Accelerator(gradient_accumulation_steps=training_args.gradient_accumulation_steps)
    # with accelerator.main_process_first():
    
    if training_args.do_load_dataset:
        logger.info("************** é¢„å¤„ç†ï¼šæ•°æ®é›† ********************")
        print_source()
        data_module, test_dataset, _, _ = maker_data_module(
            tokenizer=tokenizer,
            model=model,
            data_args=data_args,
            model_args=model_args
        )

    # logger.info("*************** 8-bit Adam é™ä½å†…å­˜æ¶ˆè€— **************")
    # """
    # 8 ä½ Adam ä¸åƒ Adafactor é‚£æ ·èšåˆä¼˜åŒ–å™¨çŠ¶æ€ï¼Œâ€‹â€‹è€Œæ˜¯ä¿ç•™å®Œæ•´çŠ¶æ€å¹¶å¯¹å…¶è¿›è¡Œé‡åŒ–ã€‚
    # é‡åŒ–æ„å‘³ç€å®ƒä»¥è¾ƒä½çš„ç²¾åº¦å­˜å‚¨çŠ¶æ€ï¼Œå¹¶ä¸”ä»…ä¸ºä¼˜åŒ–è€Œå¯¹å…¶è¿›è¡Œåé‡åŒ–ã€‚è¿™ç±»ä¼¼äº FP16 è®­ç»ƒèƒŒåçš„æƒ³æ³•ï¼Œå³ä½¿ç”¨ç²¾åº¦è¾ƒä½çš„å˜é‡å¯ä»¥èŠ‚çœå†…å­˜ã€‚
    # https://huggingface.co/docs/transformers/v4.18.0/en/performance
    # ä½¿ç”¨CPUè®­ç»ƒä¸éœ€è¦
    # """
    # decay_parameters = get_parameter_names(model, [nn.LayerNorm])
    # logger.info(decay_parameters)
    # decay_parameters = [name for name in decay_parameters if "bias" not in name]
    # logger.info(decay_parameters)
    # optimizer_grouped_parameters = [
    #     {
    #         "params": [p for n, p in model.named_parameters() if n in decay_parameters],
    #         "weight_decay": training_args.weight_decay,
    #     },
    #     {
    #         "params": [p for n, p in model.named_parameters() if n not in decay_parameters],
    #         "weight_decay": 0.0,
    #     },
    # ]
    # optimizer_kwargs = {
    #     "betas": (training_args.adam_beta1, training_args.adam_beta2),
    #     "eps": training_args.adam_epsilon,
    # }
    # optimizer_kwargs["lr"] = training_args.learning_rate
    # adam_bnb_optim = bnb.optim.Adam8bit(
    #     optimizer_grouped_parameters,
    #     betas=(training_args.adam_beta1, training_args.adam_beta2),
    #     eps=training_args.adam_epsilon,
    #     lr=training_args.learning_rate,
    # )

    if training_args.do_train:
        print_source()
        if training_args.do_accelerator:
            logger.info("************** ä½¿ç”¨ accelertaor æ„å»ºè®­ç»ƒç¯å¢ƒï¼Œå¹¶è¿›è¡Œè®­ç»ƒ **************")
            # dataloader = DataLoader(datasets, batch_size=training_args.per_device_train_batch_size)

            # if training_args.gradient_checkpointing:
            #     model.gradient_checkpointing_enable()
            
            # accelerator = Accelerator(fp16=training_args.fp16)
            # model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)

            # model.train()
            # for step, batch in enumerate(dataloader, start=1):
            #     loss = model(**batch).loss
            #     # gradient_accumulation_steps: åœ¨æ‰§è¡Œå‘å/æ›´æ–°ä¼ é€’ä¹‹å‰ç´¯ç§¯æ¢¯åº¦çš„æ›´æ–°æ­¥éª¤æ•°ã€‚
            #     loss = loss / training_args.gradient_accumulation_steps
            #     accelerator.backward(loss)
            #     if step % training_args.gradient_accumulation_steps == 0:
            #         optimizer.step()
            #         optimizer.zero_grad()
            
            # # ä½¿ç”¨wait_for_everyone()ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åœ¨ç»§ç»­ä¹‹å‰åŠ å…¥è¯¥ç‚¹ã€‚
            # accelerator.wait_for_everyone()
            # # åœ¨ä¿å­˜ä¹‹å‰ä½¿ç”¨unwrap_model()æ¥åˆ é™¤åœ¨åˆ†å¸ƒå¼è¿‡ç¨‹ä¸­æ·»åŠ çš„æ‰€æœ‰ç‰¹æ®Šæ¨¡å‹åŒ…è£…å™¨ã€‚
            # unwrapped_model = accelerator.unwrap_model(model)
            # state_dict = unwrapped_model.state_dict()
            # logger.info(state_dict)
            # # ä¿å­˜æ¨¡å‹ï¼ˆåªä¿å­˜ä¸€æ¬¡ï¼‰
            # accelerator.save(state_dict.state_dict(), training_args.clone_repo)

        else:
            """
            æˆ‘ä»¬æƒ³åœ¨è®­ç»ƒæœŸé—´è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚ Trainer é€šè¿‡æä¾› compute_metrics æ¥æ”¯æŒè®­ç»ƒæœŸé—´çš„è¯„ä¼°ã€‚
            è¯„ä¼°æ‘˜è¦ä»»åŠ¡æœ€å¸¸ç”¨çš„æŒ‡æ ‡æ˜¯ rogue_scoreï¼Œæ˜¯é¢å‘å¬å›çš„ Understudy for Gisting Evaluation çš„ç¼©å†™ï¼‰ã€‚ 
            è¯¥æŒ‡æ ‡çš„è¡Œä¸ºä¸æ ‡å‡†å‡†ç¡®åº¦ä¸åŒï¼šå®ƒå°†ç”Ÿæˆçš„æ‘˜è¦ä¸ä¸€ç»„å‚è€ƒæ‘˜è¦è¿›è¡Œæ¯”è¾ƒ
            """
            logger.info("************** ä½¿ç”¨ Trainer æ„å»ºè®­ç»ƒç¯å¢ƒ, å¹¶è¿›è¡Œè®­ç»ƒ ******************")
            if training_args.metric_type == "seqeval":
                logger.info("åŠ è½½seqevalåŒ…å«å¤šä¸ªæŒ‡æ ‡ï¼ˆç²¾åº¦ã€å‡†ç¡®æ€§ã€F1 å’Œå¬å›ç‡ï¼‰çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åºåˆ—æ ‡è®°ä»»åŠ¡ã€‚")
                metric = evaluate.load(training_args.metric_type)
                label_list = [
                    "aiå†™çš„",
                    "äººç±»å†™çš„",
                ]
                # è¯„ä¼° roc_auc
                roc_auc_score = evaluate.load("roc_auc")
                refs=[1, 0, 1, 1, 0, 0]
                pred_scores=[0.5, 0.2, 0.99, 0.3, 0.1, 0.7]
                results = roc_auc_score.compute(references=refs, prediction_scores=pred_scores)
                logger.info(round(results['roc_auc'], 2))

                def compute_metrics(pred):
                    logger.debug(f"pred:\n{pred}")
                    labels = pred.label_ids
                    preds = pred.predictions.argmax(-1)
                    logger.debug(f"new labels: {labels}")
                    logger.debug(f"new preds: {preds}")
                    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
                    acc = accuracy_score(labels, preds)
                    return {
                        'accuracy': acc,
                        'f1': f1,
                        'precision': precision,
                        'recall': recall
                    }
            else:
                logger.info("åŠ è½½accuracy")
                # Metric
                metric=evaluate.load("accuracy", "f1")
                # metric = evaluate.load("accuracy", "f1", "glue", "mrpc")
                # å°†åœ¨æ•´ä¸ªé¢„æµ‹/æ ‡ç­¾æ•°ç»„çš„æ¯ä¸ªè¯„ä¼°é˜¶æ®µç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ï¼Œä»¥ç”ŸæˆæŒ‡æ ‡ã€‚
                def compute_metrics(eval_pred):
                    # é¢„æµ‹å’Œæ ‡ç­¾è¢«åˆ†ç»„åœ¨ä¸€ä¸ªåä¸º EvalPrediction çš„å‘½åå…ƒç»„ä¸­
                    logits, labels = eval_pred
                    # è·å–é¢„æµ‹åˆ†æ•°æœ€é«˜çš„ç´¢å¼•ï¼ˆå³é¢„æµ‹æ ‡ç­¾ï¼‰
                    predictions = np.argmax(logits, axis=-1)
                    # å°†é¢„æµ‹æ ‡ç­¾ä¸å‚è€ƒæ ‡ç­¾è¿›è¡Œæ¯”è¾ƒ
                    results = metric.compute(predictions=predictions, references=labels)
                    # ç»“æœï¼šä¸€ä¸ªåŒ…å«å­—ç¬¦ä¸²é”®ï¼ˆæŒ‡æ ‡åç§°ï¼‰å’Œæµ®ç‚¹æ•°çš„å­—å…¸
                    # å€¼ï¼ˆå³æŒ‡æ ‡å€¼ï¼‰
                    return results

            # Create Trainer instance
            # åˆ›å»º Trainer å®ä¾‹
            trainer=Trainer(
                model=model,
                args=training_args,
                tokenizer=tokenizer,
                compute_metrics=compute_metrics,
                **data_module
                # å¼€å¯ int8 adam
                # optimizers=(adam_bnb_optim, None),
            )

            print_source()
            if training_args.do_visualization:
                logger.info("åŠ è½½tensorboardè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ï¼Œä¼šå½±å“æ€§èƒ½ï¼Œç”Ÿæˆç¯å¢ƒå»ºè®®å…³é—­")
                # åˆå§‹åŒ–æ—¶é—´ï¼Œç”¨äºè®¡ç®—è®­ç»ƒæ‰€ç”¨æ—¶é—´
                # start = time.perf_counter()
                # å¼€å¯pytorch profileç”¨äºç›‘æ§æ¨¡å‹æ€§èƒ½ï¼Œé€šè¿‡ tensorboard è¿›è¡ŒæŸ¥çœ‹
                with torch.profiler.profile(
                    activities=[
                        torch.profiler.ProfilerActivity.CPU, 
                        torch.profiler.ProfilerActivity.CUDA
                    ], 
                    schedule=torch.profiler.schedule(skip_first=3, wait=1, warmup=1, active=2, repeat=2),
                    # æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼Œé€šè¿‡ tensorboard è¿›è¡ŒæŸ¥çœ‹
                    on_trace_ready=torch.profiler.tensorboard_trace_handler(f"{training_args.clone_repo}/logs/resnet"),
                    profile_memory=True,
                    with_stack=True,
                    record_shapes=True
                ) as prof:
                    # æ·»åŠ è®­ç»ƒçš„å›è°ƒ
                    trainer.add_callback(ProfCallback(prof=prof))
                    train_result = trainer.train()

                logger.info("********* è¯»å–æ¨¡å‹ç»“æ„å¹¶ä¿å­˜ ****************")
                encoding_input=tokenizer(
                    test_dataset[0][data_args.index_input],
                    return_tensors="pt"
                )
                logger.info(f"encoding_input: {encoding_input}")
                # è¯¥ä»£ç çš„æœ¬æ¥ç›®çš„æ˜¯è·å–æ¨¡å‹ç»“æ„ï¼Œä½†æ˜¯ç›®å‰æ— æ³•ä¸ huggingface è¿›è¡Œæ•´åˆ
                # tb_writer=SummaryWriter(log_dir=f"{training_args.clone_repo}/logs/rjxai_graph")
                # tb_writer.add_image("train_input", encoding_input)
                # é”™è¯¯ï¼šRuntimeError: Type 'Tuple[str, str]' cannot be traced. Only Tensors and  (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced
                # tb_writer.add_graph(model, encoding_input)

                logger.info("ç›®å‰æ— æ³•åœ¨sagemakerä¸Šapt-get graphvizå¯¼è‡´æ— æ³•ä½¿ç”¨è¯¥åº“ï¼Œéœ€è¦å¯»æ‰¾å¦å¤–çš„æ–¹æ¡ˆ")
                # è®¾ç½®æ¨¡å‹çš„ä¿å­˜æ ¼å¼ä¸ºpng
                # graphviz.set_jupyter_format('png')
                # ç”Ÿæˆæ¨¡å‹ç»“æ„
                # model_graph=draw_graph(
                #     model, 
                #     input_data=encoding_input, 
                #     save_graph=True
                # )
                # visual_graph=model_graph.visual_graph
                # logger.info(visual_graph)
                # visual_graph.render(filename=f"{training_args.clone_repo}/logs/model_png")

                logger.info(f"åŸå§‹æ•°æ®é›†ï¼š{test_dataset[0][data_args.index_input]}")
                logger.info(f"encoding input: {encoding_input}")
                logger.info(encoding_input.input_ids.numpy())
                logger.info(encoding_input.input_ids.numpy().shape)
                logger.info(encoding_input.input_ids.numpy().ndim)
                
                tokens=tokenizer.convert_ids_to_tokens(encoding_input.input_ids.numpy()[0])
                logger.info(f"å°† ids è½¬æ¢æˆ tokens æ–‡æœ¬ï¼šids_to_tokens = {tokens}")
                ids=tokenizer.convert_tokens_to_ids(tokens)
                logger.info(f"å°†tokensè½¬æ¢æˆidsï¼štokens_to_ids = {ids}")
            else:
                logger.info("ä½¿ç”¨å¿«é€Ÿè®­ç»ƒ")
                train_result = trainer.train()

            logger.info(f"è®­ç»ƒç»“æœï¼š{train_result}")
            logger.info("******** æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ **********")
            print_summary(train_result)

            logger.info("******** è¿è¡Œè¯„ä¼°å¾ªç¯å¹¶è¿”å›æŒ‡æ ‡ **********")
            # å¾ˆå¥½ï¼Œæˆ‘ä»¬å·²ç»è®­ç»ƒäº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚ ğŸ‰è®©æˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šå†æ¬¡è¯„ä¼°æœ€ä½³æ¨¡å‹ã€‚
            eval_result = trainer.evaluate()
            logger.info(eval_result)

    if training_args.do_predict:
        logger.info("************ è¿”å›æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ï¼ˆå¦‚æœæ ‡ç­¾å¯ç”¨ï¼Œåˆ™å¸¦æœ‰åº¦é‡ï¼‰ ********************")
        # è¿è¡Œæ¨ç†
        # eval_result = trainer.evaluate(eval_dataset=tokenized_dataset["test"])
        # logger.info(eval_result)
        # # å°†è¯„ä¼°ç»“æœå†™å…¥æ–‡ä»¶ï¼Œç¨åå¯ä»¥åœ¨ s3 è¾“å‡ºä¸­è®¿é—®è¯¥æ–‡ä»¶
        # with open(os.path.join(f"{repository_id}/logs", "eval_results.txt"), "w") as writer:
        #     print(f"***** Eval results *****")
        #     for key, value in sorted(eval_result.items()):
        #         writer.write(f"{key} = {value}\n")
        
        # æŸ¥çœ‹æµ‹è¯•é›†å¤§å°
        logger.info(test_dataset)
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        test_prediction = trainer.predict(test_dataset)
        # å¯¹äºæ¯ä¸ªé¢„æµ‹ï¼Œä½¿ç”¨ argmax åˆ›å»ºæ ‡ç­¾ï¼ˆè·å–ç¬¬ä¸€åˆ—çš„æ ‡ç­¾
        logger.info(test_prediction)
        
        logger.info(test_prediction.predictions.shape)
        logger.info(test_prediction.label_ids.shape)
        logger.info(test_prediction[0])
        
        test_predictions_argmax = np.argmax(test_prediction[0], axis=1)
        # ä»æµ‹è¯•é›†ä¸­æ£€ç´¢å‚è€ƒæ ‡ç­¾
        test_references=np.array(test_dataset["labels"])
        logger.info(test_predictions_argmax)
        logger.info(test_references)
        # è®¡ç®—ç²¾åº¦
        results = metric.compute(predictions=test_predictions_argmax, references=test_references)
        logger.info(f"test ç²¾åº¦å€¼ï¼š{results}")


    # model.save_pretrained(training_args.clone_repo, push_to_hub=True)
    model.push_to_hub(training_args.clone_repo, use_auth_token=model_args.write_hf_hub_token)
    tokenizer.push_to_hub(training_args.clone_repo, use_auth_token=model_args.write_hf_hub_token)

    if training_args.do_save:
        # ä¿å­˜æˆ‘ä»¬çš„åˆ†è¯å™¨å¹¶åˆ›å»ºæ¨¡å‹å¡
        logger.info(training_args.clone_repo)
        trainer.save_model(training_args.clone_repo)

        # login(token=model_args.write_hf_hub_token, add_to_git_credential=True)
        # trainer.push_to_hub(training_args.clone_repo)
        with training_args.main_process_first(desc="save training model"):
            # with accelerator.main_process_first():
            logger.info("******* ä¿å­˜æ¨¡å‹ huggingface hub ********")
            # ä¿å­˜åˆ° huggingface
            # TODOï¼šå»¶è¿Ÿä¿å­˜ä¼šå­˜åœ¨å¤¸å¤©è®­ç»ƒæ— æ³•ä¿å­˜çš„é—®é¢˜ï¼ˆè¯¥æœºåˆ¶æœ‰å¾…è¿›ä¸€æ­¥å®Œå–„ï¼‰
            hf_host="https://huggingface.co"
            # ä¸Šä¼ æœ€æ–°çš„æ¨¡å‹åˆ°å­˜å‚¨åº“
            logger.info("*** save model hub ***")
            api=HfApi()
            list_models=api.list_models(
                search=training_args.clone_repo, 
                token=model_args.hf_hub_token
            )
            if len(list_models) == 0:
                # åˆ›å»ºè¿œç¨‹å­˜å‚¨åº“ï¼ˆåç»­åˆ¤æ–­æ˜¯å¦éœ€è¦ï¼Ÿï¼‰
                create_repo(
                    training_args.clone_repo, 
                    private=True,
                    token=model_args.write_hf_hub_token
                )
            else:
                logger.info(list_models[0])

            # æµ‹è¯•ä½¿ç”¨ä¸Šé¢çš„ä¿å­˜æŸ¥çœ‹æ•ˆæœ
            # ä¸Šä¼ ç›®å½•ä¸‹çš„æ–‡ä»¶åˆ°å­˜å‚¨åº“
            logger.info(f"folder_path:{training_args.clone_repo}")
            list_dir=os.listdir(training_args.clone_repo)
            logger.info(list_dir)
            api.upload_folder(
                folder_path=training_args.clone_repo,
                repo_id=training_args.clone_repo,
                token=model_args.write_hf_hub_token
            )
            # æŸ¥è¯¢å­˜å‚¨åº“å¹¶æ‰“å°(å†…å®¹)
            logger.info(f"{hf_host}/{training_args.clone_repo}")

if __name__ == "__main__":
    single_task_train()